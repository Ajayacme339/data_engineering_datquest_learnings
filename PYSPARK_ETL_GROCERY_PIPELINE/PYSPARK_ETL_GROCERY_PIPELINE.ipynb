{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8d415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28 entries, 0 to 27\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   order_id      28 non-null     object\n",
      " 1   customer_id   27 non-null     object\n",
      " 2   product_name  28 non-null     object\n",
      " 3   price         28 non-null     object\n",
      " 4   quantity      28 non-null     int64 \n",
      " 5   order_date    28 non-null     object\n",
      " 6   region        28 non-null     object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"DATA/RAW/online_orders.csv\")\n",
    "#df.head(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d63404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"GroceryPipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b158e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------------------+-----+--------+----------+------+\n",
      "|order_id|customer_id|product_name         |price|quantity|order_date|region|\n",
      "+--------+-----------+---------------------+-----+--------+----------+------+\n",
      "|ORD_1001|CUST_5421  |Organic Bananas      |$2.99|3       |2024-10-15|West  |\n",
      "|ORD_1002|8823       |Whole Milk           |3.49 |2       |2024-10-16|East  |\n",
      "|ORD_1003|CUST_9912  |Sourdough Bread      |$4.50|1       |2024-10-16|West  |\n",
      "|ORD_1004|TEST_USER  |TEST PRODUCT - IGNORE|$0.00|1       |2024-10-16|Test  |\n",
      "|ORD_1005|3344       |Greek Yogurt         |5.99 |4       |2024-10-17|South |\n",
      "+--------+-----------+---------------------+-----+--------+----------+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-------+--------+-----------------+-------------+-----------------+------------------+----------+------+\n",
      "|summary|order_id|      customer_id| product_name|            price|          quantity|order_date|region|\n",
      "+-------+--------+-----------------+-------------+-----------------+------------------+----------+------+\n",
      "|  count|      28|               27|           28|               28|                28|        28|    28|\n",
      "|   mean|    NULL|5450.090909090909|         NULL|6.546666666666668|2.1785714285714284|      NULL|  NULL|\n",
      "| stddev|    NULL|2903.050824720279|         NULL|3.290919324444159|1.3348205990866457|      NULL|  NULL|\n",
      "|    min|ORD_1001|             1122|Almond Butter|            $0.00|                 1|2024-10-15|  East|\n",
      "|    max|ORD_1027|        TEST_USER|   Whole Milk|             9.99|                 6|2024-10-28|  West|\n",
      "+-------+--------+-----------------+-------------+-----------------+------------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"DATA/RAW/online_orders.csv\", header=True)\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2f4d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "result_customer_id = df.filter(df.customer_id.contains(\"TEST\")).count()\n",
    "print(result_customer_id)\n",
    "result_price = df.filter(df.price.contains('$')).count()\n",
    "print(result_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446c94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Grocery_Daily_ETL\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def extract_sales_data(spark,input_path):\n",
    "    \"\"\" Reading Raw Sales Data \"\"\"\n",
    "    logger.info(f\"Reading Raw Sales Data from {input_path}\")\n",
    "\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"order_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"price\", StringType(), True),\n",
    "        StructField(\"quantity\", StringType(), True),\n",
    "        StructField(\"order_date\", StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df = spark.read.csv(\n",
    "        input_path,\n",
    "        header=True,\n",
    "        schema=expected_schema,\n",
    "        mode=\"PERMISSIVE\"\n",
    "    )\n",
    "\n",
    "    total_records = df.count()\n",
    "    logger.info(f\"Found {total_records} total records\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dc9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_data(spark):\n",
    "    \"\"\"Combine data from multiple sources\"\"\"\n",
    "    #Each Sytem exports differently \n",
    "    online_orders = extract_all_data(spark,\"data/raw/online_orders.csv\")\n",
    "    mobile_orders = extract_all_data(spark,\"data/raw/mobile_orders.csv\")\n",
    "    store_orders = extract_all_data(spark,\"data/raw/store_orders.csv\")\n",
    "    # Combine them together using union all \n",
    "    all_orders = online_orders.unionByName(store_orders).unionByName(mobile_orders)\n",
    "    logger.info(f\"Combined dataset has {all_orders.count()} orders\")\n",
    "    return all_orders\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825d54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_id(df):\n",
    "    \"\"\"Standardize customer IDs (some are numbers, some are CUST_123 format)\"\"\"\n",
    "    df_cleaned = df.withColumn(\n",
    "        \"customer_id_cleaned\",\n",
    "        when(\n",
    "            col(\"customer_id\").startswith(\"CUST_\"), \n",
    "            col(\"customer_id\")\n",
    "        ).when(\n",
    "            col(\"customer_id\").rlike(\"^[0-9]+$\"), \n",
    "            concat(lit(\"CUST_\"), col(\"customer_id\"))\n",
    "        ).otherwise(\n",
    "            col(\"customer_id\")\n",
    "        )\n",
    "    )\n",
    "    return df_cleaned.drop(\"customer_id\").withColumnRenamed(\"customer_id_cleaned\", \"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ad9d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_price_column(df):\n",
    "    \"\"\"Fix the Price column by removing $ and converting to float\"\"\"\n",
    "   # Remove dollar signs, commas, etc. (keep digits, decimals, and negatives)\n",
    "    df_cleaned = df.withColumn(\n",
    "        \"price_cleaned\",\n",
    "        regexp_replace(col(\"price\"), r\"[^0-9.\\-]\", \"\")\n",
    "    )\n",
    "\n",
    "    # Convert to decimal ,default to 0 if it fails\n",
    "    df_final = df_cleaned.withColumn(\n",
    "        \"price_decimal\",\n",
    "        when(col(\"price_cleaned\").isNotNull(),\n",
    "             col(\"price_cleaned\").cast(DoubleType()))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "      # Flag suspicious values for review\n",
    "    df_flagged = df_final.withColumn(\n",
    "        \"price_quality_flag\",\n",
    "        when(col(\"price_decimal\") == 0.0, \"CHECK_ZERO_PRICE\")\n",
    "        .when(col(\"price_decimal\") > 1000, \"CHECK_HIGH_PRICE\")\n",
    "        .when(col(\"price_decimal\") < 0, \"CHECK_NEGATIVE_PRICE\")\n",
    "        .otherwise(\"OK\")\n",
    "    )\n",
    "\n",
    "    bad_price_count = df_flagged.filter(col(\"price_quality_flag\") != \"OK\").count()\n",
    "    logger.warning(f\"Found {bad_price_count} orders with suspicious prices\")\n",
    "\n",
    "    return df_flagged.drop(\"price\", \"price_cleaned\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f1e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dates(df):\n",
    "    \"\"\"Parse dates in multiple common formats\"\"\"\n",
    "\n",
    "    fmt1 = to_date(col(\"order_date\", \"yyyy-MM-dd\"))\n",
    "    fmt2 = to_date(col(\"order_date\", \"MM/dd/yyyy\"))\n",
    "    fmt3 = to_date(col(\"order_date\", \"dd-MM-yyyy\"))\n",
    "\n",
    "    df_parsed = df.withColumn(\n",
    "        \"order_date_parsed\",\n",
    "        coalesce(fmt1, fmt2, fmt3)\n",
    "    )\n",
    "\n",
    "     # Check how many we couldn't parse\n",
    "    unparsed = df_parsed.filter(col(\"order_date_parsed\").isNull()).count()\n",
    "    if unparsed > 0:\n",
    "        logger.warning(f\"Could not parse {unparsed} dates\")\n",
    "\n",
    "    return df_parsed.drop(\"order_date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac801fc3",
   "metadata": {},
   "source": [
    "#### Removing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01088601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_test_data(df):\n",
    "    \"\"\"Remove Test Data orders that somehow made it to production (case-insensitive)\"\"\"\n",
    "    df_filtered = df.filter(\n",
    "        ~(\n",
    "            (upper(col(\"customer_id\")).contains(\"test\")) |\n",
    "            (upper(col(\"product_name\")).contains(\"test\")) | \n",
    "            col(\"customer_id\").isNull() |\n",
    "            col(\"order_id\").isNull()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    removed_count = df.count() - df_filtered.count()\n",
    "    logger.info(f\"Removed {removed_count} test/invalid orders\")\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b72141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_duplicates(df):\n",
    "    \"\"\"Remove duplicte records from the data\"\"\"\n",
    "    df_deduped = df.drop_duplicates([\"order_id\"])\n",
    "    duplicate_count = df.count() - df_deduped.count()\n",
    "    if duplicate_count > 0:\n",
    "        logger.info(f\"Removed {duplicate_count} duplicate orders\")\n",
    "\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "665432b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transform_orders(df):\n",
    "    \"\"\"Apply all transformations in sequence\"\"\"\n",
    "\n",
    "    logger.info(\"Starting data transformation...\")\n",
    "\n",
    "    # Clean each aspect of the data\n",
    "    df = clean_customer_id(df)\n",
    "    df = clean_price_column(df)\n",
    "    df = standardize_dates(df)\n",
    "    df = remove_test_data(df)\n",
    "    df = handle_duplicates(df)\n",
    "\n",
    "    # Cast quantity to integer\n",
    "    df = df.withColumn(\n",
    "        \"quantity\",\n",
    "        when(col(\"quantity\").isNotNull(), col(\"quantity\").cast(IntegerType()))\n",
    "        .otherwise(1)\n",
    "    )\n",
    "\n",
    "    # Add some useful calculated fields\n",
    "    df = df.withColumn(\"total_amount\", col(\"price_decimal\") * col(\"quantity\")) \\\n",
    "           .withColumn(\"processing_date\", current_date()) \\\n",
    "           .withColumn(\"year\", year(col(\"order_date_parsed\"))) \\\n",
    "           .withColumn(\"month\", month(col(\"order_date_parsed\")))\n",
    "\n",
    "    # Rename for clarity\n",
    "    df = df.withColumnRenamed(\"order_date_parsed\", \"order_date\") \\\n",
    "           .withColumnRenamed(\"price_decimal\", \"unit_price\")\n",
    "\n",
    "    logger.info(f\"Transformation complete. Final record count: {df.count()}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927f9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/etl_pipeline.py (continuing in same file)\n",
    "\n",
    "def load_to_csv(spark, df, output_path):\n",
    "    \"\"\"Save processed data for downstream use\"\"\"\n",
    "\n",
    "    logger.info(f\"Writing {df.count()} records to {output_path}\")\n",
    "\n",
    "    #Convert to Pandas\n",
    "    pandas_df = df.toPandas()\n",
    "\n",
    "    # Create output directory if needed\n",
    "    import os\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    output_file = f\"{output_path}/orders.csv\"\n",
    "    pandas_df.to_csv(output_file, index=False)\n",
    "\n",
    "    logger.info(f\"Successfully wrote {len(pandas_df)} records\")\n",
    "    logger.info(f\"Output location: {output_file}\")\n",
    "\n",
    "    return len(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0674f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/etl_pipeline.py (continuing in same file)\n",
    "\n",
    "def sanity_check_data(spark, output_path):\n",
    "    \"\"\"Quick validation using Spark SQL\"\"\"\n",
    "\n",
    "    # Read the CSV file back\n",
    "    output_file = f\"{output_path}/orders.csv\"\n",
    "    df = spark.read.csv(output_file, header=True, inferSchema=True)\n",
    "    df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "    # Run some quick validation queries\n",
    "    total_count = spark.sql(\"SELECT COUNT(*) as total FROM orders\").collect()[0]['total']\n",
    "    logger.info(f\"Sanity check - Total orders: {total_count}\")\n",
    "\n",
    "    # Check for any suspicious data that slipped through\n",
    "    zero_price_count = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*) as zero_prices\n",
    "        FROM orders\n",
    "        WHERE unit_price = 0\n",
    "    \"\"\").collect()[0]['zero_prices']\n",
    "\n",
    "    if zero_price_count > 0:\n",
    "        logger.warning(f\"Found {zero_price_count} orders with zero price\")\n",
    "\n",
    "    # Verify date ranges make sense\n",
    "    date_range = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            MIN(order_date) as earliest,\n",
    "            MAX(order_date) as latest\n",
    "        FROM orders\n",
    "    \"\"\").collect()[0]\n",
    "\n",
    "    logger.info(f\"Date range: {date_range['earliest']} to {date_range['latest']}\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c61462",
   "metadata": {},
   "source": [
    "### Create a Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a00320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/etl_pipeline.py (continuing in same file)\n",
    "\n",
    "def create_summary_report(df):\n",
    "    \"\"\"Generate metrics about the ETL run\"\"\"\n",
    "\n",
    "    summary = {\n",
    "        \"total_orders\": df.count(),\n",
    "        \"unique_customers\": df.select(\"customer_id\").distinct().count(),\n",
    "        \"unique_products\": df.select(\"product_name\").distinct().count(),\n",
    "        \"total_revenue\": df.agg(sum(\"total_amount\")).collect()[0][0],\n",
    "        \"date_range\": f\"{df.agg(min('order_date')).collect()[0][0]} to {df.agg(max('order_date')).collect()[0][0]}\",\n",
    "        \"regions\": df.select(\"region\").distinct().count()\n",
    "    }\n",
    "\n",
    "    logger.info(\"\\n=== ETL Summary Report ===\")\n",
    "    for key, value in summary.items():\n",
    "        logger.info(f\"{key}: {value}\")\n",
    "    logger.info(\"========================\\n\")\n",
    "\n",
    "    return summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
